{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This repo allows you to train a Parlai chatbot with the Chinese T5 model released by UNER.\n",
    "#### You can also replace the Chinese T5 model with your own T5 model simply by replacing the tokenizer and model path\n",
    "#### The code shown in this notebook is basically copied from the the original T5 repo\n",
    "#### I made some minor changes in order to make it work with the Chinesr T5 model\n",
    "#### I ommit details about how to prepare the training data because of laziness. \n",
    "#### Please go to my previous repo called  \"parlai_chinese_chatbot_by_gpt2\" for more information about data preparation\n",
    "#### Any question please send me email : studyouwei@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load all the necessary libraries\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from parlai.agents.hugging_face.dict import HuggingFaceDictionaryAgent\n",
    "from parlai.agents.hugging_face.t5 import (ParlaiT5Decoder, ParlaiT5Encoder,\n",
    "                                           ParlaiT5Model, T5Agent)\n",
    "from parlai.core.agents import register_agent\n",
    "from parlai.core.opt import Opt\n",
    "from parlai.core.teachers import DialogTeacher, register_teacher\n",
    "from parlai.core.torch_generator_agent import TorchGeneratorModel\n",
    "from parlai.scripts.train_model import TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers.models.t5.modeling_t5 import T5Stack\n",
    "except ModuleNotFoundError:\n",
    "    T5Stack = object\n",
    "# please disable this env variable in case any unexpected problems occur\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define your own 'parlai' teacher which is responsible for feeding your model training samples\n",
    "\n",
    "\n",
    "data = pd.read_csv('train_data.csv')\n",
    "@register_teacher(\"my_teacher\")\n",
    "class MyTeacher(DialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "       \n",
    "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "    def setup_data(self, datafile):\n",
    "      \n",
    "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
    "\n",
    "       \n",
    "        for _, diag in data.iterrows():\n",
    "            text = diag['txt']\n",
    "            labels = diag['label']\n",
    "            start = diag['start']\n",
    "            if isinstance(text, str) and isinstance(labels, str):\n",
    "                # print(text)\n",
    "                # print(labels)\n",
    "                yield (text, labels), start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define your tokenizer for the Chinese t5 model\n",
    "class MyDictionaryAgent(HuggingFaceDictionaryAgent):\n",
    "    def get_tokenizer(self, opt):\n",
    "        return BertTokenizer.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
    "\n",
    "    @property\n",
    "    def add_special_tokens(self) -> bool:\n",
    "        \"\"\"\n",
    "        Whether to add special tokens when tokenizing.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def skip_decode_special_tokens(self) -> bool:\n",
    "        \"\"\"\n",
    "        Whether to add special tokens when tokenizing.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def override_special_tokens(self, opt):\n",
    "        # now override\n",
    "        self.start_token = self.hf_tokenizer.cls_token\n",
    "        self.end_token = self.hf_tokenizer.sep_token\n",
    "        self.null_token = self.hf_tokenizer.pad_token\n",
    "        self.unk_token = self.hf_tokenizer.unk_token\n",
    "\n",
    "        self._unk_token_idx = self.hf_tokenizer.unk_token_id\n",
    "        self.start_idx = self.hf_tokenizer.cls_token_id\n",
    "        self.end_idx = self.hf_tokenizer.sep_token_id\n",
    "        self.null_idx = self.hf_tokenizer.pad_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Wrap t5 models, including its encoder and decoder, in Parlai format\n",
    "\n",
    "class MyParlaiT5Model(TorchGeneratorModel):\n",
    "    \"\"\"\n",
    "    Wrap T5 in ParlAI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt, dictionary):\n",
    "        self.pad_idx = dictionary[dictionary.null_token]\n",
    "        self.start_idx = self.pad_idx\n",
    "        self.end_idx = dictionary[dictionary.end_token]\n",
    "        super().__init__(self.pad_idx, self.start_idx, self.end_idx)\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"uer/t5-small-chinese-cluecorpussmall\")\n",
    "\n",
    "        self.encoder = ParlaiT5Encoder(\n",
    "            opt, self.t5.get_encoder(), self.pad_idx)\n",
    "        self.decoder = ParlaiT5Decoder(\n",
    "            opt, self.t5.get_decoder(), self.pad_idx)\n",
    "        self.paralleled = not opt['t5_model_parallel']\n",
    "\n",
    "    def _get_initial_forced_decoder_input(self, bsz: int, inputs: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Return initial input to the decoder.\n",
    "        :param bsz:\n",
    "            batchsize\n",
    "        :param inputs:\n",
    "            inputs to decode\n",
    "        :return initial_input:\n",
    "            initial input for the decoder.\n",
    "        \"\"\"\n",
    "        inputs = torch.cat([self.START.detach().expand(bsz, 1), inputs], 1)\n",
    "        return inputs\n",
    "\n",
    "    def reorder_encoder_states(self, encoder_states, indices):\n",
    "        \"\"\"\n",
    "        Reorder the encoder states.\n",
    "        See ``TorchGeneratorModel.reorder_encoder_states`` for a description.\n",
    "        \"\"\"\n",
    "        enc, mask = encoder_states\n",
    "        if not torch.is_tensor(indices):\n",
    "            indices = torch.LongTensor(indices).to(enc.device)\n",
    "        enc = torch.index_select(enc, 0, indices)\n",
    "        mask = torch.index_select(mask, 0, indices)\n",
    "        return enc, mask\n",
    "\n",
    "    def reorder_decoder_incremental_state(\n",
    "        self, incremental_state: Dict[int, dict], inds: torch.Tensor\n",
    "    ) -> Dict[int, dict]:\n",
    "        \"\"\"\n",
    "        Not *quite* sure how to reconcile this with HF.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def output(self, tensor):\n",
    "        \"\"\"\n",
    "        Compute output logits.\n",
    "        \"\"\"\n",
    "        tensor = tensor * (self.t5.model_dim**-0.5)\n",
    "        lm_logits = self.t5.lm_head(tensor)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "class ParlaiT5Encoder(torch.nn.Module):\n",
    "    def __init__(self, opt: Opt, encoder: T5Stack, padding_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.stack = encoder\n",
    "        self.padding_idx = padding_idx\n",
    "        self.paralleled = not opt[\n",
    "            't5_model_parallel'\n",
    "        ]  # need to parallel in forward; bug in HF\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.LongTensor,\n",
    "        positions: Optional[torch.LongTensor] = None,\n",
    "        segments: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.BoolTensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param LongTensor[batch,seqlen] input:\n",
    "            The input IDs\n",
    "        :param LongTensor[batch,seqlen] positions:\n",
    "            Positions for input IDs\n",
    "        :param LongTensor[batch,seqlen] segments:\n",
    "            If provided, additionally adds ``segments`` as extra embedding features.\n",
    "        \"\"\"\n",
    "        if not self.paralleled:\n",
    "            self.stack.parallelize()\n",
    "        mask = input != self.padding_idx\n",
    "        # print(input)\n",
    "        # print(self.stack.embed_tokens)\n",
    "        outputs = self.stack(input, attention_mask=mask,\n",
    "                             output_hidden_states=False)\n",
    "        for k in outputs:\n",
    "            if torch.is_tensor(outputs[k]):\n",
    "                outputs[k] = outputs[k].to(input.device)\n",
    "        return outputs[0], mask\n",
    "\n",
    "\n",
    "class ParlaiT5Decoder(torch.nn.Module):\n",
    "    def __init__(self, opt: Opt, decoder: T5Stack, padding_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.stack = decoder\n",
    "        self.padding_idx = padding_idx\n",
    "        self.paralleled = not opt[\n",
    "            't5_model_parallel'\n",
    "        ]  # need to parallel in forward; bug in HF\n",
    "\n",
    "    def forward(\n",
    "        self, input: torch.LongTensor, encoder_state: Tuple[Any], incr_state=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param LongTensor[batch,seqlen] input:\n",
    "            The decoder inputs (partial or full decoded token IDs).\n",
    "        :param encoder_state:\n",
    "            Output from the encoder module forward pass.\n",
    "        :param incr_state:\n",
    "            The incremental state: a dictionary whose keys index the layers and whose\n",
    "            values contain the incremental state for each layer.\n",
    "        \"\"\"\n",
    "        if not self.paralleled:\n",
    "            self.stack.parallelize()\n",
    "        encoder_output, encoder_mask = encoder_state\n",
    "\n",
    "        mask = input != self.padding_idx\n",
    "        mask[:, 0] = True  # first token is pad\n",
    "        # print(input)\n",
    "\n",
    "        outputs = self.stack(\n",
    "            input_ids=input,\n",
    "            attention_mask=mask,\n",
    "            encoder_hidden_states=encoder_output.to(input.device),\n",
    "            encoder_attention_mask=encoder_mask.to(input.device),\n",
    "        )\n",
    "        return outputs[0].to(input.device), incr_state\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define your t5 agent \n",
    "@register_agent('chinese_t5')\n",
    "class ChineseT5Agent(T5Agent):\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = MyParlaiT5Model(self.opt, self.dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_dictionary(self):\n",
    "        \"\"\"\n",
    "        Overrides TorchAgent.build_dictionary to use t5 dict.\n",
    "        \"\"\"\n",
    "        return MyDictionaryAgent(self.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now you are all set, just train the model !\n",
    "####  Please comment out the param no_cuda=True if you do have cuda available ! \n",
    "TrainModel.main(\n",
    "    model='chinese_t5',\n",
    "    model_file='./model',\n",
    "    task='my_teacher',\n",
    "    lr=1e-5,\n",
    "    optimizer='adam',\n",
    "    warmup_updates=100,\n",
    "    # t5_model_parallel = True,\n",
    "    text_truncate=512,\n",
    "    t5_model_arch='t5-small',\n",
    "    batchsize=8,\n",
    "    fp16=True,\n",
    "    num_epochs=3,\n",
    "    no_cuda=True\n",
    "    # fp16_impl='mem_efficient'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1f392defe650829700f492eb57f9ca780dd56d6bebdcdeef32d1ec2c20a25b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
